{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39612e6e",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ff190f",
   "metadata": {},
   "source": [
    "你会如何用一个简短的句子来描述 TensorFlow 呢？它的主要特点是什么？你能说出其他流行的深度学习库的名字吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1882b9",
   "metadata": {},
   "source": [
    "**答案**：\n",
    "\n",
    "TensorFlow是一个用于数值计算的开源库，特别适用于大规模机器学习和微调。它的核心与NumPy相似，但它还具有GPU支持、分布式计算支持、计算图分析和优化功能（具有可移植的图格式，允许您在一个环境中训练TensorFlow模型，并在另一个环境下运行它）、基于反向模式autodiff的优化API以及几个强大的API，如tf.keras、tf.data、tf.image、tf.signal、，以及更多。其他流行的深度学习库包括PyTorch、MXNet、Microsoft认知工具包、Theano、Caffe2和Chainer。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be279f99",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbe9bc1",
   "metadata": {},
   "source": [
    "TensorFlow 是 NumPy 的临时替代品吗？这两者之间的主要区别是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6305d002",
   "metadata": {},
   "source": [
    "**答案**：\n",
    "\n",
    "尽管TensorFlow提供了NumPy提供的大部分功能，但出于几个原因，它并不是一个替代品。\n",
    "\n",
    "1. 首先，函数的名称并不总是相同的（例如，tf.reduce_sum（）与np.sum（））。\n",
    "2. 其次，一些函数的行为方式并不完全相同（例如，tf.reduce（）创建张量的转置副本，而NumPy的T属性创建转置视图，而实际上没有复制任何数据）。\n",
    "3. 最后，NumPy数组是可变的，而TensorFlow张量不是可变的（但如果需要可变对象，可以使用tf.Variable）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fd22fa",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed971c0",
   "metadata": {},
   "source": [
    "使用 tf.range(10) 和 tf.constant(np.arange(10)) 获得的结果相同吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55dc9ef",
   "metadata": {},
   "source": [
    "**答案**：\n",
    "\n",
    "tf.range(10) 和 tf.constant(np.arange(10)) 都返回包含整数0到9的一维张量。然而，前者使用32位整数，后者使用64位整数。实际上，TensorFlow默认为32位，而NumPy默认为64位。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92926bf8",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b1524a",
   "metadata": {},
   "source": [
    "除了规则张量之外，你还能说出TensorFlow中的其他六种数据结构吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6356c6",
   "metadata": {},
   "source": [
    "**答案**：\n",
    "\n",
    "除了常规张量，TensorFlow还提供了其他几种数据结构，包括稀疏张量、张量阵列、不规则张量、队列、字符串张量和集合。最后两个实际上表示为正则张量，但TensorFlow提供了特殊的函数来处理它们（在tf.strings和tf.sets中）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d358e474",
   "metadata": {},
   "source": [
    "## Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1ef8c6",
   "metadata": {},
   "source": [
    "您可以通过编写函数或子类化 tf.keras.losses.Loss 来定义自定义损失函数。你什么时候会使用每个选项？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dadaaaa",
   "metadata": {},
   "source": [
    "**答案**：\n",
    "\n",
    "当您想要定义自定义损失函数时，通常可以将其作为常规Python函数实现。但是，如果您的自定义loss函数必须支持某些超参数（或任何其他状态），那么您应该将keras.losses.loss类子类化，并实现__init__（）和call（）方法。如果您希望损失函数的超参数与模型一起保存，那么还必须实现get_config（）方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9132a1d2",
   "metadata": {},
   "source": [
    "## Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987d047b",
   "metadata": {},
   "source": [
    "类似地，您可以在函数中定义自定义度量，或者作为 tf.keras.metrics.Metric 的子类。你什么时候会使用每个选项？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf48776f",
   "metadata": {},
   "source": [
    "**答案**：\n",
    "\n",
    "与自定义损失函数非常相似，大多数度量可以定义为常规Python函数。但是，如果您希望自定义度量支持某些超参数（或任何其他状态），那么应该将keras.metrics.metric类子类化。此外，如果在整个epoch上计算度量不等同于在该epoch中的所有batch上计算平均度量（例如，精度和召回度量），则应将keras.metrics.metric类子类化，并实现__init__（）、update_state（）和result（）方法，以跟踪每个历元期间的运行度量。您还应该实现reset_states（）方法，除非它只需要将所有变量重置为0.0。如果您希望状态与模型一起保存，那么也应该实现get_config（）方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4525d9",
   "metadata": {},
   "source": [
    "## Exercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4fa1bb",
   "metadata": {},
   "source": [
    "什么时候应该创建自定义层还是自定义模型？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f8e7fc",
   "metadata": {},
   "source": [
    "**答案**：\n",
    "\n",
    "您应该将模型的内部组件（即层或可重用的层块）与模型本身（即要训练的对象）区分开来。前者应子类keras.layers.Layer类，而后者应子类keras.models.Model类。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f9b7d0",
   "metadata": {},
   "source": [
    "## Exercise 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e91ddc",
   "metadata": {},
   "source": [
    "有哪些用例需要编写您自己的自定义训练循环？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dd0747",
   "metadata": {},
   "source": [
    "**答案**：\n",
    "\n",
    "编写自己的自定义训练循环是相当先进的，因此只有在真正需要时才应该这样做。Keras提供了几个工具来定制训练，而不必编写自定义训练循环：回调、自定义正则化、自定义约束、自定义丢失等。您应该尽可能使用这些方法，而不是编写自定义训练循环：编写自定义训练环更容易出错，而且重用您编写的自定义代码会更困难。\n",
    "\n",
    "然而，在某些情况下，编写自定义训练循环是必要的⁠—例如，如果你想对神经网络的不同部分使用不同的优化器，比如在Wide&Deep论文中。自定义训练循环在调试或试图准确理解训练的工作方式时也很有用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74655c0f",
   "metadata": {},
   "source": [
    "## Exercise 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e218f841",
   "metadata": {},
   "source": [
    "自定义Keras组件是否包含任意Python代码，或者它们是否必须可转换为TF函数？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19d2398",
   "metadata": {},
   "source": [
    "**答案**：\n",
    "\n",
    "自定义Keras组件应可转换为TF函数，这意味着它们应尽可能遵守TF操作，并遵守第12章（TF函数规则部分）中列出的所有规则。如果您绝对需要在自定义组件中包含任意Python代码，则可以将其包装在tf.py_function（）操作中（但这会降低性能并限制模型的可移植性），或者在创建自定义层或模型时设置dynamic=True（或者在调用模型的compile（）方法时设置run_eagerly=True）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd08af7e",
   "metadata": {},
   "source": [
    "## Exercise 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c02f48",
   "metadata": {},
   "source": [
    "如果您希望函数转换为TF函数，需要尊重的主要规则是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e6ef17",
   "metadata": {},
   "source": [
    "**答案**：\n",
    "\n",
    "请参阅第12章，了解创建TF功能时应遵守的规则列表（在TF功能规则部分）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46be839",
   "metadata": {},
   "source": [
    "## Exercise 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64455aca",
   "metadata": {},
   "source": [
    "你什么时候需要创建一个动态的Keras模型呢？你该怎么做呢？为什么不让你所有的模型都动态化呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4816ca2",
   "metadata": {},
   "source": [
    "**答案**：\n",
    "\n",
    "创建动态Keras模型对于调试非常有用，因为它不会将任何自定义组件编译为TF函数，您可以使用任何Python调试器来调试代码。如果您希望在模型（或培训代码）中包含任意Python代码，包括对外部库的调用，那么它也很有用。\n",
    "\n",
    "要使模型成为动态的，必须在创建模型时设置dynamic=True。或者，可以在调用模型的compile（）方法时设置run_eagerly=True。\n",
    "\n",
    "使模型动态化会阻止Keras使用TensorFlow的任何图形功能，因此它会降低训练和推理速度，并且您将无法导出计算图，这将限制模型的可移植性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2c275a",
   "metadata": {},
   "source": [
    "## Exercise 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3473936e",
   "metadata": {},
   "source": [
    "实现一个执行层规范化的自定义层（我们将在第15章中使用这种类型的层）：\n",
    "\n",
    "1. build() 方法应该定义两个可训练的权值 $α$ 和 $β$ ，形状input_shape\\[-1:\\]并且数据类型为tf.float32。$α$ 应该初始化为1，$β$ 应该初始化为0。\n",
    "2. call() 方法应该计算每个实例特征的平均 $μ$ 和标准差 $σ$ 。为此，您可以使用tf.nn.moments(inputs, axes=-1, keepdims=True)，它返回所有实例的均值 $μ$ 和方差 $σ^2$ （计算方差的平方根来得到标准差）。然后函数应该计算并返回 $α⊗（X-μ）/（σ+ε）+β$ ，其中⊗表示逐乘法（\\*），$ε$ 是一个平滑项（一个小常数以避免除零，例如，0.001）。\n",
    "3. 确保自定义图层生成与tf.keras.layers.LayerNormalization相同（或非常接近相同）的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ad0094",
   "metadata": {},
   "source": [
    "**答案**："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c341b3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from time import strftime\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8e122f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the CIFAR10 dataset\n",
    "\n",
    "cifar10 = tf.keras.datasets.cifar10.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = cifar10\n",
    "\n",
    "X_train = X_train_full[5000:]\n",
    "y_train = y_train_full[5000:]\n",
    "X_valid = X_train_full[:5000]\n",
    "y_valid = y_train_full[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ec1d530",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(tf.keras.layers.Layer):\n",
    "    def __init__(self, eps=0.001, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        self.alpha = self.add_weight(\n",
    "            name=\"alpha\", shape=batch_input_shape[-1:],\n",
    "            initializer=\"ones\")\n",
    "        self.beta = self.add_weight(\n",
    "            name=\"beta\", shape=batch_input_shape[-1:],\n",
    "            initializer=\"zeros\")\n",
    "        super().build(batch_input_shape) # must be at the end\n",
    "\n",
    "    def call(self, X):\n",
    "        mean, variance = tf.nn.moments(X, axes=-1, keepdims=True)\n",
    "        return self.alpha * (X - mean) / (tf.sqrt(variance + self.eps)) + self.beta\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"eps\": self.eps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c443fe81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=6.5335115e-07>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create one instance of each class\n",
    "# apply them to some data (e.g., the training set)\n",
    "# and ensure that the difference is negligeable.\n",
    "\n",
    "X = X_train.astype(np.float32)\n",
    "\n",
    "custom_layer_norm = LayerNormalization()\n",
    "keras_layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "tf.reduce_mean(tf.keras.losses.mean_absolute_error(\n",
    "    keras_layer_norm(X), custom_layer_norm(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814a7ca3",
   "metadata": {},
   "source": [
    "## Exercise 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1ba891",
   "metadata": {},
   "source": [
    "使用一个自定义的训练循环来训练一个模型，以处理Fashion MNIST数据集。（见第10章）：\n",
    "\n",
    "1. 显示每个epoch、迭代、平均训练损失和平均准确性（每次迭代时更新），以及每个历元结束时的验证损失和准确性。\n",
    "2. 尝试对上层和下层使用不同的优化器和不同的学习速率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fdbe52",
   "metadata": {},
   "source": [
    "**答案**："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf55e1e",
   "metadata": {},
   "source": [
    "**（a）**："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0283c674",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "X_train_full = X_train_full.astype(np.float32) / 255.\n",
    "\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "X_test = X_test.astype(np.float32) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a4adf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "759d542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "006a00ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.01)\n",
    "loss_fn = tf.keras.losses.sparse_categorical_crossentropy\n",
    "mean_loss = tf.keras.metrics.Mean()\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "306949cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12d9ccf9d9f41538c8b7b14f43066f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "All epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f5cf8b0d2de4993aa705e807373489f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/1718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e3b6498690440ca62dfcb58b005ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|          | 0/1718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f6a75f6f574a65875e24017af882aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|          | 0/1718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d3e49e8dbb48cc9cf1e752289fd73d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5:   0%|          | 0/1718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16d75b02a08453a84c2df740f3eb463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5:   0%|          | 0/1718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import trange\n",
    "from collections import OrderedDict\n",
    "\n",
    "def random_batch(X, y, batch_size=32):\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "with trange(1, n_epochs + 1, desc=\"All epochs\") as epochs:\n",
    "    for epoch in epochs:\n",
    "        with trange(1, n_steps + 1, desc=f\"Epoch {epoch}/{n_epochs}\") as steps:\n",
    "            for step in steps:\n",
    "                X_batch, y_batch = random_batch(X_train, y_train)\n",
    "                with tf.GradientTape() as tape:\n",
    "                    y_pred = model(X_batch)\n",
    "                    main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "                    loss = tf.add_n([main_loss] + model.losses)\n",
    "                gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "                for variable in model.variables:\n",
    "                    if variable.constraint is not None:\n",
    "                        variable.assign(variable.constraint(variable))                    \n",
    "                status = OrderedDict()\n",
    "                mean_loss(loss)\n",
    "                status[\"loss\"] = mean_loss.result().numpy()\n",
    "                for metric in metrics:\n",
    "                    metric(y_batch, y_pred)\n",
    "                    status[metric.name] = metric.result().numpy()\n",
    "                steps.set_postfix(status)\n",
    "            y_pred = model(X_valid)\n",
    "            status[\"val_loss\"] = np.mean(loss_fn(y_valid, y_pred))\n",
    "            status[\"val_accuracy\"] = np.mean(tf.keras.metrics.sparse_categorical_accuracy(\n",
    "                tf.constant(y_valid, dtype=np.float32), y_pred))\n",
    "            steps.set_postfix(status)\n",
    "        for metric in [mean_loss] + metrics:\n",
    "            metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f27e9de",
   "metadata": {},
   "source": [
    "**（b）**："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f64e52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dcc3e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_layers = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "])\n",
    "upper_layers = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "model = tf.keras.Sequential([\n",
    "    lower_layers, upper_layers\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d98f4eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
    "upper_optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ecc0f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "loss_fn = tf.keras.losses.sparse_categorical_crossentropy\n",
    "mean_loss = tf.keras.metrics.Mean()\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e65b7bf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9584ef5c4a45718eff980eca5f2ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "All epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d6e0e891134604a3172b966bc3f3f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/1718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7648bbf47b4d22ab9907de2546d9f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|          | 0/1718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea46f09b3cf4841af2809c3a9f50700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|          | 0/1718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d033733c384de880c9ad0ee6459b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5:   0%|          | 0/1718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfc6a624f53048b9aa785f2d6181b9ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5:   0%|          | 0/1718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with trange(1, n_epochs + 1, desc=\"All epochs\") as epochs:\n",
    "    for epoch in epochs:\n",
    "        with trange(1, n_steps + 1, desc=f\"Epoch {epoch}/{n_epochs}\") as steps:\n",
    "            for step in steps:\n",
    "                X_batch, y_batch = random_batch(X_train, y_train)\n",
    "                with tf.GradientTape(persistent=True) as tape:\n",
    "                    y_pred = model(X_batch)\n",
    "                    main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "                    loss = tf.add_n([main_loss] + model.losses)\n",
    "                for layers, optimizer in ((lower_layers, lower_optimizer),\n",
    "                                          (upper_layers, upper_optimizer)):\n",
    "                    gradients = tape.gradient(loss, layers.trainable_variables)\n",
    "                    optimizer.apply_gradients(zip(gradients, layers.trainable_variables))\n",
    "                del tape\n",
    "                for variable in model.variables:\n",
    "                    if variable.constraint is not None:\n",
    "                        variable.assign(variable.constraint(variable))                    \n",
    "                status = OrderedDict()\n",
    "                mean_loss(loss)\n",
    "                status[\"loss\"] = mean_loss.result().numpy()\n",
    "                for metric in metrics:\n",
    "                    metric(y_batch, y_pred)\n",
    "                    status[metric.name] = metric.result().numpy()\n",
    "                steps.set_postfix(status)\n",
    "            y_pred = model(X_valid)\n",
    "            status[\"val_loss\"] = np.mean(loss_fn(y_valid, y_pred))\n",
    "            status[\"val_accuracy\"] = np.mean(tf.keras.metrics.sparse_categorical_accuracy(\n",
    "                tf.constant(y_valid, dtype=np.float32), y_pred))\n",
    "            steps.set_postfix(status)\n",
    "        for metric in [mean_loss] + metrics:\n",
    "            metric.reset_states()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
